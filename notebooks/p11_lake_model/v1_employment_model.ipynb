{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Employment-Unemployment Model\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "Estimate a Markov chain using employment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps to \"understanding the world around us\" (a short digression)\n",
    "\n",
    "\n",
    "Our friend Jim Savage has thought about what a \"modern statistical workflow\" entails. We summarize some steps he has proposed for understanding the world around us:\n",
    "\n",
    "> 1. Prepare and visualize your data.\n",
    "> 2. Create a generative model for the data...\n",
    ">   - A first model should be as simple as possible.\n",
    "> 3. Simulate some artificial data from your model given some assumed parameters that you \"pick out of a hat\" ($\\theta$)\n",
    "> 4. Use the artificial data to estimate the model parameters ($\\hat{\\theta})$\n",
    "> 5. Check that you recovered a good approximation of the \"known unknowns\" (aka, $\\theta \\approx \\hat{\\theta}$)\n",
    ">   - Possibly repeat 3-5 with different estimators and true parameters ($\\theta$), to get an understanding of how well the fitting procedure works\n",
    "> 6. Fit the model to your real data, check the fit\n",
    "> 7. Argue about the results with your friends and colleagues\n",
    "> 8. Go back to 2. with a slightly richer model. Repeat.\n",
    "> 9. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis... Note that in many cases, we will do step 9 before steps 1-8!\n",
    "\n",
    "Later this semester, we will talk formally about what it means to \"fit\" your model (and the work that it entails), but, for now, we find it sufficient to say that it's a process to ensure that the probability distribution over outcomes generated by your model lines up with the data (aka, finding the right model parameters).\n",
    "\n",
    "We'll do a version of steps 1-6 to help us improve our understanding of the labor data that we previously saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our plan\n",
    "\n",
    "1. Prepare and visualize our data.\n",
    "2. Develop a generative model of employment and unemployment\n",
    "3. Simulate data from our generative model for given parameters\n",
    "4. Fit our model to the simulated data\n",
    "5. Explore different ways that we might have chosen to fit the data\n",
    "6. Fit the model with the BLS data\n",
    "7. Examine what our model implies for the effects of COVID on employment/unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Prepare and visualize our data\n",
    "\n",
    "We have done this in earlier lectures and will not repeat the work here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Create a generative model\n",
    "\n",
    "**A simple model of employment**\n",
    "\n",
    "In the vein of, \"the first model created should be as simple as possible\", we use the employment model that we studied earlier.\n",
    "\n",
    "Consider a single individual that transitions between employment and unemployment\n",
    "\n",
    "* When unemployed, they find a new job with probability $\\alpha$\n",
    "* When employed, they lose their job with probability $\\beta$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![ModelFlowchart](model_diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Simulate data from our generative model\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will break simulating data from the model into two steps:\n",
    "\n",
    "1. Given today's state and the transition probabilities, draw from tomorrow's state\n",
    "2. Given an initial state and transition probabilities, simulate an entire history of employment/unemployment using the one-step transition kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulate the one-step employment transition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alfa and beta are float type here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def next_state(s_t, alpha, beta):\n",
    "    \"\"\"\n",
    "    Transitions from employment/unemployment in period t to\n",
    "    employment/unemployment in period t+1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s_t : int\n",
    "        The individual's current state... s_t = 0 maps to\n",
    "        unemployed and s_t = 1 maps to employed\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    s_tp1 : int\n",
    "        The individual's employment state in `t+1`\n",
    "    \"\"\"\n",
    "    # follow good coding habits and write down the documentation\n",
    "\n",
    "    # Draw a random number\n",
    "    u_t = np.random.rand() # 0=unemployed, 1=employed\n",
    "\n",
    "    # Let 0 be unemployed... If unemployed and draws --> s_t = 0\n",
    "    # a value less than lambda then becomes employed --> when is she going to be employed again\n",
    "    if (s_t == 0) and (u_t < alpha):\n",
    "        s_tp1 = 1\n",
    "    # Let 1 be employed... If employed and draws a\n",
    "    # value less than beta then becomes unemployed\n",
    "    elif (s_t == 1) and (u_t < beta):\n",
    "        s_tp1 = 0\n",
    "    # Otherwise, he keeps the same state as he had\n",
    "    # at period t\n",
    "    else:\n",
    "        s_tp1 = s_t\n",
    "\n",
    "    return s_tp1\n",
    "\n",
    "    # this function depends on the markov property\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Notice how this function incorporates the Markov property that our model assumes.\n",
    "\n",
    "The Markov property says $\\text{Probability}(s_{t+1} | s_{t}) = \\text{Probability}(s_{t+1} | s_{t}, s_{t-1}, \\dots, s_0)$.\n",
    "\n",
    "This means that, other than the transition probabilities, we only need to know today's state and not the entire history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Testing our function**\n",
    "\n",
    "It's always a good idea to write some simple test cases for functions that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state(0, 0.5, 0.5)\n",
    "\n",
    "# The individual's current state... s_t = 0 maps to unemployed and s_t = 1 maps to employed\n",
    "# The probability that an individual goes from unemployed to employed\n",
    "#        The probability that an individual goes from employed to unemployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should never become employed from unemployment\n",
    "# if alpha is 0\n",
    "next_state(0, 0.0, 0.5) == 0\n",
    "\n",
    "# she should stay unemployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should never become unemployed from employment\n",
    "# if beta is 0\n",
    "next_state(1, 0.5, 0.0) == 1\n",
    "\n",
    "# should stay employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should always transition to employment from unemployment\n",
    "# when alpha is 1\n",
    "next_state(0, 1.0, 0.5) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should always transition to unemployment from employment\n",
    "# when beta is 1\n",
    "next_state(1, 0.5, 1.0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulate entire history**\n",
    "\n",
    "**Note**: Later we will allow $\\alpha$ and $\\beta$ to change over time, so while we want you think of them as constant for now, we will write code that allows for them to fluctate period-by-period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want alfa and beta to fluctuate period by period\n",
    "# alfa and beta are numpy array type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_employment_history(alpha, beta, s_0):\n",
    "    \"\"\"\n",
    "    Simulates the history of employment/unemployment. It\n",
    "    will simulate as many periods as elements in `alpha`\n",
    "    and `beta`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    s_0 : int\n",
    "        The initial state of unemployment/employment, which\n",
    "        should take value of 0 (unemployed) or 1 (employed)\n",
    "    \"\"\"\n",
    "    # Create array to hold the values of our simulation\n",
    "    # we got to make sure that alfa and beta are from the same length\n",
    "    \n",
    "    assert(len(alpha) == len(beta))\n",
    "    T = len(alpha)\n",
    "    s_hist = np.zeros(T+1, dtype=int)\n",
    "\n",
    "    s_hist[0] = s_0\n",
    "    for t in range(T):\n",
    "        # Step one period into the future\n",
    "        s_0 = next_state(s_0, alpha[t], beta[t])  # Notice alpha[t] and beta[t]\n",
    "        s_hist[t+1] = s_0\n",
    "\n",
    "    return s_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Check output of the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to use the function, we need to know whether the person was employed or unemployed\n",
    "# set first value of the function with the previous state\n",
    "\n",
    "alpha = np.ones(50)*0.25\n",
    "beta = np.ones(50)*0.025\n",
    "\n",
    "# probability of losing a job can is lower than the prob of finding a job\n",
    "\n",
    "simulate_employment_history(alpha, beta, 0)\n",
    "\n",
    "# i can put 0 or 1 in the simulate function to see what happens\n",
    "# thus it doesnt take long for ther to find a job ([period 2])\n",
    "# thus our simulated method has succeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Fit your model to our artificial data\n",
    "\n",
    "There are lots of procedures that one could use to infer parameters values from data. Here we'll just count relative frequencies of transitions.\n",
    "\n",
    "Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\n",
    "\n",
    "$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$\n",
    "\n",
    "Let $\\{y_0, y_1, \\dots, y_T\\}$ be a sequence of observations generated from the $N$-state Markov chain, then our \"fitting\" procedure would assign the following value to $p_{ij}$:\n",
    "\n",
    "$$p_{ij} = \\frac{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i} \\mathbb{1}_{y_{t+1} == j}}{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i}}$$\n",
    "\n",
    "**Note**: If you'd like to understand why this procedure makes sense, we recommend computing $\\sum_{j=1}^N p_{ij}$ for a given $i$. What value do you get? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to fit out simu;ated model to artifiocial data\n",
    "# p11 means that agent that is in state 1 is going to be in state 1 next period and so on till Nth term\n",
    "# p12 means that agent that is in state 1 is going to be in state 2 next period and so on till Nth term\n",
    "\n",
    "# example from video\n",
    "    # artificial data : 0111001\n",
    "    # it takes 1 if the condition below is true, or zero otherwise\n",
    "    # over the summation of the places where yt was equal to 0\n",
    "    # then we have 1.0+1.1+0.1/1+1+1= 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, lets write some code that can count frequences\n",
    "# THE PROBABILITY OF TRANSISIOTING FROM STATE 0 TO 1 AND VICEVERSA\n",
    "# all the possible states = idx--> 01234... T\n",
    "# History is going to give me TRUE FALSE TRUE etc\n",
    "# idx < T-1)--> TRUE TRUE TRUE... FALSE at T\n",
    "# SEE THE NOW BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Counting frequencies**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def count_frequencies_individual(history):\n",
    "    \"\"\"\n",
    "    Computes the transition probabilities for a two-state\n",
    "    Markov chain--> # therefore we are going to specialize\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : np.array(int, ndim=1)\n",
    "        An array with the state values of a two-state Markov chain\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        The probability of transitioning from state 0 to 1\n",
    "    beta : float\n",
    "        The probability of transitioning from state 1 to 0\n",
    "    \"\"\"\n",
    "    # Get length of the simulation and an index tracker\n",
    "    T = len(history)\n",
    "    idx = np.arange(T)\n",
    "\n",
    "\n",
    "# this step is to find where the zeros and the ones are\n",
    "    # Determine when the chain had values 0 and 1 -- Notice\n",
    "    # that we can't use the last value because we don't see\n",
    "    # where it transitions to\n",
    "    zero_idxs = idx[(history == 0) & (idx < T-1)] # THIS IS GOING TO GIVE US SOME TRUES AND FALSES\n",
    "    one_idxs = idx[(history == 1) & (idx < T-1)] # THIS IS GOING TO GIVE US SOME TRUES AND FALSES\n",
    "\n",
    "# here im using the information of where the zeros and the ones are, to figure out what the corresponding counts of frequencies should be\n",
    "    # Check what percent of the t+1 values were 0/1\n",
    "    # HOW DOES THE COUNTING PROCEDUREE WORK\n",
    "    alpha = np.sum(history[zero_idxs+1]) / len(zero_idxs) # NOW WE ARE GOING TO SUM UP THE HISTORY FOR ALPHA\n",
    "    beta = np.sum(1 - history[one_idxs+1]) / len(one_idxs) # THEN WE DO IT \n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Checking the fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to check the accuracy of fitting out model to a simulated model (the count of frequencies model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def check_accuracy(T, alpha=0.25, beta=0.025):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of our fit by printing the true values\n",
    "    and the fitted values for a given T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : int\n",
    "        The length of our simulation\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    \"\"\"\n",
    "    idx = np.arange(T)\n",
    "    alpha_np = np.ones(T)*alpha\n",
    "    beta_np = np.ones(T)*beta\n",
    "\n",
    "    # Simulate a sample history\n",
    "    emp_history = simulate_employment_history(alpha_np, beta_np, 0)\n",
    "\n",
    "    # Check the fit\n",
    "    alpha_hat, beta_hat = count_frequencies_individual(emp_history)\n",
    "    \n",
    "\n",
    "    # finally we peint out the true values and the fitted values\n",
    "    print(f\"True alpha was {alpha} and fitted value was {alpha_hat}\")\n",
    "    print(f\"True beta was {beta} and fitted value was {beta_hat}\")\n",
    "    \n",
    "    return alpha, alpha_hat, beta, beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.25028312570781425\n",
      "True beta was 0.025 and fitted value was 0.02424042996599759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.25028312570781425, 0.025, 0.02424042996599759)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(10_000, 0.25, 0.025)\n",
    "\n",
    "# we find that we are getting correct results that are very similar to our initial parameters--> 0.25 and 0.025\n",
    "# that is good. it means that we had enough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Well... If we observe 10,000 months of employment history for someone then we know that we can back out the parameters of our models...\n",
    "\n",
    "Unfortunately, our real world data won't have that much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if if we have less data than 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about for an entire lifetime of employment transitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.31666666666666665\n",
      "True beta was 0.025 and fitted value was 0.0375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.31666666666666665, 0.025, 0.0375)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(45*12, 0.25, 0.025)\n",
    "\n",
    "# this is particularly inaccurate 0.316666666\n",
    "# there could have been structural changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about for just two years of observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.5\n",
      "True beta was 0.025 and fitted value was 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.5, 0.025, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(2*12, 0.25, 0.025)\n",
    "\n",
    "# two years of data, the fitting procedure can be very noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE COULD APPLY CROSS SECTIONAL ANALYSIS BUT WE WOULD NEED INDEPENDENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3 and 4 (second try)\n",
    "\n",
    "Data for the employment history of a single individual will not give us a good chance of fitting our model accurately...\n",
    "\n",
    "However, the BLS isn't infering EU/UE rates from its observation of a single individual. Rather, they're using a cross-section of individuals!\n",
    "\n",
    "Can we use a cross-section rather than for one individual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, but, in order for a version of our \"frequency counting\" procedure to work, we want independence across individuals, i.e.\n",
    "\n",
    "$$\\text{Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \\alpha, \\beta) = \\text{Probability}(s_{i, t+1} | s_{i, t}, \\alpha, \\beta) \\text{Probability}(s_{j, t+1} | s_{j, t}, \\alpha, \\beta)$$\n",
    "\n",
    "When we observed only a single individual, the Markov property did a lot of the work to get independence for us.\n",
    "\n",
    "When might this not be the case?\n",
    "\n",
    "* Change in government policy results in a \"jobs guarantee\"\n",
    "* Technological change results in the destruction of an entire industries jobs\n",
    "* Recession causes increased firing across entire country\n",
    "\n",
    "(Spoiler alert: Some of these will present problems for us... which is why we'll allow for $\\alpha$ and $\\beta$ to move each period)\n",
    "\n",
    "(TODO: Tom said he'd like to edit this cell again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \\alpha, \\beta)\n",
    "\n",
    "# THIS APPEARRS ABOVE\n",
    "# joint distribution of states conditional on the iniitial joint distribution plus the initial states = products of their independent distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to simulate the cross section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulating a cross-section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_employment_cross_section(alpha, beta, s_0, N=500):\n",
    "    \"\"\"\n",
    "    Simulates a cross-section of employment/unemployment using\n",
    "    the model we've described above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    s_0 : np.array(int, ndim=1)\n",
    "        The fraction of the population that begins in each\n",
    "        employment state\n",
    "    N : int\n",
    "        The number of individuals in our cross-section\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_hist_cs : np.array(int, ndim=2)\n",
    "        An `N x T` matrix that contains an individual\n",
    "        history of employment along each row\n",
    "    \"\"\"\n",
    "    # we are going to store the different states in a big matrix\n",
    "\n",
    "\n",
    "    # Make sure transitions are same size and get the length\n",
    "    # of the simulation from the length of the transition\n",
    "    # probabilities\n",
    "    assert(len(alpha) == len(beta))\n",
    "    T = len(alpha)\n",
    "\n",
    "    # Check the fractions add to one and figure out how many\n",
    "    # zeros we should have\n",
    "    assert(np.abs(np.sum(s_0) - 1.0) < 1e-8)\n",
    "    Nz = np.floor(s_0[0]*N).astype(int)\n",
    "\n",
    "    # Allocate space to store the simulations\n",
    "    s_hist_cs = np.zeros((N, T+1), dtype=int)\n",
    "    s_hist_cs[Nz:, 0] = 1\n",
    "    \n",
    "    for i in range(N):\n",
    "        s_hist_cs[i, :] = simulate_employment_history(\n",
    "            alpha, beta, s_hist_cs[i, 0]\n",
    "        )\n",
    "    \n",
    "    return s_hist_cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = np.ones(1)*0.25\n",
    "beta = np.ones(1)*0.025\n",
    "\n",
    "simulate_employment_cross_section(alpha, beta, np.array([0.35, 0.65]), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Store the simulation in pandas**\n",
    "\n",
    "Real world data will typically be stored in a DataFrame, so let's store our artificial data in a DataFrame as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def pandas_employment_cross_section(eu_ue_df, s_0, N=500):\n",
    "    \"\"\"\n",
    "    Simulate a cross-section of employment experiences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eu_ue_df : pd.DataFrame\n",
    "        A DataFrame with columns `dt`, `alpha`, and `beta`\n",
    "        that have the monthly eu/ue transition rates\n",
    "    s_0 : np.array(float, ndim=1)\n",
    "        The fraction of the population that begins in each\n",
    "        employment state\n",
    "    N : int\n",
    "        The numbers of individuals in our cross-section\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame with the dates and an employment outcome\n",
    "        associated with each date of `eu_ue_df`\n",
    "    \"\"\"\n",
    "    # Make sure that `ue_ue_df` is sorted by date\n",
    "    eu_ue_df = eu_ue_df.sort_values(\"dt\")\n",
    "    alpha = eu_ue_df[\"alpha\"].to_numpy()\n",
    "    beta = eu_ue_df[\"beta\"].to_numpy()\n",
    "\n",
    "    # we will extract our alpha and data arrays\n",
    "\n",
    "    # Simulate cross-section\n",
    "    employment_history = simulate_employment_cross_section(\n",
    "        alpha, beta, s_0, N\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(employment_history[:, :-1].T)\n",
    "    df = pd.concat([eu_ue_df[\"dt\"], df], axis=1)\n",
    "    df = pd.melt(\n",
    "        df, id_vars=[\"dt\"],\n",
    "        var_name=\"pid\", value_name=\"employment\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>pid</th>\n",
       "      <th>employment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt pid  employment\n",
       "0  2018-01-01   0           0\n",
       "1  2018-02-01   0           0\n",
       "2  2018-03-01   0           1\n",
       "3  2018-04-01   0           1\n",
       "4  2018-05-01   0           1\n",
       "5  2018-06-01   0           1\n",
       "6  2018-07-01   0           1\n",
       "7  2018-08-01   0           1\n",
       "8  2018-09-01   0           1\n",
       "9  2018-10-01   0           1\n",
       "10 2018-11-01   0           1\n",
       "11 2018-12-01   0           1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 24\n",
    "eu_ue_df = pd.DataFrame(\n",
    "    {\n",
    "        \"dt\": pd.date_range(\"2018-01-01\", periods=T, freq=\"MS\"), \n",
    "        \"alpha\": np.ones(T)*0.25,\n",
    "        \"beta\": np.ones(T)*0.025\n",
    "    }\n",
    ")\n",
    "\n",
    "df = pandas_employment_cross_section(eu_ue_df, np.array([0.25, 0.75]), N=5_000)\n",
    "df.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulating the CPS**\n",
    "\n",
    "Just to \"keep it interesting\", let's tie our hands in a similar way to how the BLS has their hands tied.\n",
    "\n",
    "We will simulate an individual's full employment history, but will only keep the subset that corresponds to when they would have been interviewed by the CPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cps_interviews(df, start_year, start_month):\n",
    "    \"\"\"\n",
    "    Takes an individual simulated employment/unemployment\n",
    "    history and \"interviews\" the individual as if they were\n",
    "    in the CPS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame with at least the columns `pid`, `dt`,\n",
    "        and `employment`\n",
    "    start_year : int\n",
    "        The year in which their interviewing begins\n",
    "    start_month : int\n",
    "        The month in which their interviewing begins\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cps : pd.DataFrame\n",
    "        A DataFrame with the same columns as `df` but only\n",
    "        with observations that correspond to the CPS\n",
    "        interview schedule for someone who starts\n",
    "        interviewing in f`{start_year}/{start_month}`\n",
    "    \"\"\"\n",
    "    # Get dates that are associated with being interviewed in\n",
    "    # the CPS\n",
    "    start_date_y1 = dt.datetime(start_year, start_month, 1)\n",
    "    dates_y1 = pd.date_range(start_date_y1, periods=4, freq=\"MS\")\n",
    "    start_date_y2 = dt.datetime(start_year+1, start_month, 1)\n",
    "    dates_y2 = pd.date_range(start_date_y2, periods=4, freq=\"MS\")\n",
    "    dates = dates_y1.append(dates_y2)\n",
    "\n",
    "    # FROM start_date_y1 TO start_date_y2\n",
    "\n",
    "    # Filter data that's not in the dates\n",
    "    cps = df.loc[df[\"dt\"].isin(dates), :]\n",
    "\n",
    "    return cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interview = lambda x: cps_interviews(\n",
    "    x,\n",
    "    np.random.choice(x[\"dt\"].dt.year.unique()),\n",
    "    np.random.randint(1, 13)\n",
    ")\n",
    "\n",
    "cps_data = (\n",
    "    df.groupby(\"pid\")\n",
    "      .apply(\n",
    "          lambda x: interview(x)\n",
    "      )\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How many people are we observing per month?**\n",
    "\n",
    "If we think about the pattern used for the CPS interviews, we can form an idea of how many people might be interviewed each month.\n",
    "\n",
    "Consider if we started interviewing $m$ new individuals per month. How many would we be interviewing in any given month?\n",
    "\n",
    "Well. We'd at least be interviewing the $m$ new individuals. We would also be interviewing all of the individuals that had started their interviews in the previous 3 months. Additionally, we would be interviewing all of the individuals who had begun their interviews during those four months of the previous year.\n",
    "\n",
    "We can see this below -- Note that our \"survey\" begins in January 2018, so at first we only have $m$ individuals being interviewed, but, as the survey progresses, we move towards $8 m$ individuals being interviewed each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-01     224\n",
       "2018-02-01     424\n",
       "2018-03-01     640\n",
       "2018-04-01     853\n",
       "2018-05-01     859\n",
       "2018-06-01     862\n",
       "2018-07-01     868\n",
       "2018-08-01     848\n",
       "2018-09-01     830\n",
       "2018-10-01     831\n",
       "2018-11-01     832\n",
       "2018-12-01     855\n",
       "2019-01-01    1074\n",
       "2019-02-01    1266\n",
       "2019-03-01    1466\n",
       "2019-04-01    1663\n",
       "2019-05-01    1660\n",
       "2019-06-01    1687\n",
       "2019-07-01    1709\n",
       "2019-08-01    1707\n",
       "2019-09-01    1676\n",
       "2019-10-01    1650\n",
       "2019-11-01    1642\n",
       "2019-12-01    1630\n",
       "Name: dt, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps_data[\"dt\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fitting to a cross-section**\n",
    "\n",
    "Well... Now our data look exactly like what the BLS uses to estimate the EU and UE transition rates from the raw data.\n",
    "\n",
    "In order to fit the data, we are going to continue using the \"frequency of transition\" concept that we previously proposed, but, we must account for the shape of the data we receive now.\n",
    "\n",
    "Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\n",
    "\n",
    "$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$\n",
    "\n",
    "Let $\\{ \\{y_{i, 0}, y_{i, 1}, \\dots, y_{i, T_i}\\} \\; \\forall i \\in \\{0, 1, \\dots, I\\}\\}$ be a $I$ sequences of observations generated from the $N$-state Markov chain, then our new \"fitting\" procedure would assign the following value to $p_{ij}$:\n",
    "\n",
    "$$p_{ij} = \\frac{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i} \\mathbb{1}_{y_{m, t+1} == j}}{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cross-sectional counting frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cps_count_frequencies(df):\n",
    "    \"\"\"\n",
    "    Estimates the transition probability from employment\n",
    "    and unemployment histories of a CPS sample of\n",
    "    individuals\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A sample of individuals from the CPS survey. Must\n",
    "        have columns `dt`, `pid`, and `employment`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        The probability of transitioning from unemployment\n",
    "        to employment\n",
    "    beta : float\n",
    "        The probability of transitioning from employment\n",
    "        to unemployment\n",
    "    \"\"\"\n",
    "    # Set the index to be dt/pid\n",
    "    data_t = df.set_index([\"dt\", \"pid\"])\n",
    "\n",
    "    # Now find the \"t+1\" months and \"pid\"s\n",
    "    tp1 = data_t.index.get_level_values(\"dt\").shift(periods=1, freq=\"MS\")\n",
    "    pid = data_t.index.get_level_values(\"pid\")\n",
    "    idx = pd.MultiIndex.from_arrays([tp1, pid], names=[\"dt\", \"pid\"])\n",
    "\n",
    "    # Now \"index\" into the data and reset index\n",
    "    data_tp1 = (\n",
    "        data_t.reindex(idx)\n",
    "            .rename(columns={\"employment\": \"employment_tp1\"})\n",
    "    )\n",
    "    out = pd.concat(\n",
    "        [\n",
    "            data_t.reset_index().loc[:, [\"dt\", \"pid\", \"employment\"]],\n",
    "            data_tp1.reset_index()[\"employment_tp1\"]\n",
    "        ], axis=1, sort=True\n",
    "    ).dropna(subset=[\"employment_tp1\"])\n",
    "    out[\"employment_tp1\"] = out[\"employment_tp1\"].astype(int)\n",
    "\n",
    "    # Count how frequently we go from 0 to 1\n",
    "    out_zeros = out.query(\"employment == 0\")\n",
    "    alpha = out_zeros[\"employment_tp1\"].mean()\n",
    "    \n",
    "    # Count how frequently we go from 1 to 0\n",
    "    out_ones = out.query(\"employment == 1\")\n",
    "    beta = (1 - out_ones[\"employment_tp1\"]).mean()\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Checking accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def check_accuracy_cs(N, T, alpha=0.25, beta=0.025):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of our fit by printing the true values\n",
    "    and the fitted values for a given T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The total number of people we ever interview\n",
    "    T : int\n",
    "        The length of our simulation\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    \"\"\"\n",
    "    alpha_beta_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dt\": pd.date_range(\"2018-01-01\", periods=T, freq=\"MS\"), \n",
    "            \"alpha\": np.ones(T)*alpha,\n",
    "            \"beta\": np.ones(T)*beta\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Simulate the full cross-section\n",
    "    frac_unemployed = beta / (alpha + beta)\n",
    "    frac_employed = alpha / (alpha + beta)\n",
    "    df = pandas_employment_cross_section(\n",
    "        alpha_beta_df, np.array([frac_unemployed, frac_employed]), N\n",
    "    )\n",
    "\n",
    "    # Interview individuals according to the cps interviews\n",
    "    interview = lambda x: cps_interviews(\n",
    "        x,\n",
    "        np.random.choice(df[\"dt\"].dt.year.unique()),\n",
    "        np.random.randint(1, 13)\n",
    "    )\n",
    "    cps_data = (\n",
    "        df.groupby(\"pid\")\n",
    "          .apply(\n",
    "              lambda x: interview(x)\n",
    "          )\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Check the fit\n",
    "    alpha_hat, beta_hat = cps_count_frequencies(cps_data)\n",
    "    \n",
    "    print(f\"True alpha was {alpha} and fitted value was {alpha_hat}\")\n",
    "    print(f\"True beta was {beta} and fitted value was {beta_hat}\")\n",
    "    \n",
    "    return alpha, alpha_hat, beta, beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.23714285714285716\n",
      "True beta was 0.025 and fitted value was 0.026960110041265476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.23714285714285716, 0.025, 0.026960110041265476)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy_cs(1_000, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.2698412698412698\n",
      "True beta was 0.025 and fitted value was 0.029601722282023683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.2698412698412698, 0.025, 0.029601722282023683)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy_cs(500, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha was 0.25 and fitted value was 0.23809523809523808\n",
      "True beta was 0.025 and fitted value was 0.02476780185758514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.23809523809523808, 0.025, 0.02476780185758514)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy_cs(100, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 7: Fit the model with actual CPS data\n",
    "\n",
    "We've downloaded (and cleaned!) a subset of real CPS data for the years 2018 and 2019.\n",
    "\n",
    "Let's see what our constant parameter model does with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cps_data.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8d3acf61e2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load real CPS data that we've cleaned for you.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcps_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cps_data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[1;32m    458\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 )\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mpath_or_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cps_data.parquet'"
     ]
    }
   ],
   "source": [
    "# Load real CPS data that we've cleaned for you.\n",
    "cps_data = pd.read_parquet(\"cps_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What does this data contain?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Finding an employment history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_count_sum = cps_data.groupby(\"pid\").agg(\n",
    "    {\"dt\": \"count\", \"employment\": \"sum\"}\n",
    ").sort_values(\"dt\")\n",
    "\n",
    "cps_count_sum.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.query(\"pid == 20180602828701\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Find an individual who experiences unemployment?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_count_sum.query(\"(dt == 8) & (employment < 8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.query(\"pid == 20180307173302\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Computing $\\alpha$ and $\\beta$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha_cps, beta_cps = cps_count_frequencies(cps_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "beta_cps / (alpha_cps + beta_cps)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "31582b9feba862c420bc95ad7fac43fb721c474490d1710b4e50ac63470f9531"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
